{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c89610e-7560-4cd4-89c2-a61f1296d741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6abc1efc-0f7c-4a1c-a6cd-051695bafae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebba0fe5-dd99-4867-b707-6be17ca9dabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import uuid\n",
    "from typing import (\n",
    "    Annotated,\n",
    "    Any,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    List,\n",
    "    Literal,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Type,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    AnyMessage,\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    ToolCall,\n",
    ")\n",
    "from langchain_core.prompt_values import PromptValue\n",
    "from langchain_core.runnables import (\n",
    "    Runnable,\n",
    "    RunnableLambda,\n",
    ")\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ValidationNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7db74b57-83a2-4608-890f-bfde450266df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _default_aggregator(messages: Sequence[AnyMessage]) -> AIMessage:\n",
    "    for m in messages[::-1]:\n",
    "        if m.type == \"ai\":\n",
    "            return m\n",
    "    raise ValueError(\"No AI message found in the sequence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e892a5f-c3e6-4b11-a771-02aca6574e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetryStrategy(TypedDict, total=False):\n",
    "    \"\"\"The retry strategy for a tool call.\"\"\"\n",
    "\n",
    "    max_attempts: int\n",
    "    \"\"\"The maximum number of attempts to make.\"\"\"\n",
    "    fallback: Optional[\n",
    "        Union[\n",
    "            Runnable[Sequence[AnyMessage], AIMessage],\n",
    "            Runnable[Sequence[AnyMessage], BaseMessage],\n",
    "            Callable[[Sequence[AnyMessage]], AIMessage],\n",
    "        ]\n",
    "    ]\n",
    "    \"\"\"The function to use once validation fails.\"\"\"\n",
    "    aggregate_messages: Optional[Callable[[Sequence[AnyMessage]], AIMessage]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1841860c-bf01-4ff2-a3c4-702c7b71ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bind_validator_with_retries(\n",
    "    llm: Union[\n",
    "        Runnable[Sequence[AnyMessage], AIMessage],\n",
    "        Runnable[Sequence[BaseMessage], BaseMessage],\n",
    "    ],\n",
    "    *,\n",
    "    validator: ValidationNode,\n",
    "    retry_strategy: RetryStrategy,\n",
    "    tool_choice: Optional[str] = None,\n",
    ") -> Runnable[Union[List[AnyMessage], PromptValue], AIMessage]:\n",
    "    \"\"\"Binds a tool validators + retry logic to create a runnable validation graph.\n",
    "\n",
    "    LLMs that support tool calling can generate structured JSON. However, they may not always\n",
    "    perfectly follow your requested schema, especially if the schema is nested or has complex\n",
    "    validation rules. This method allows you to bind a validation function to the LLM's output,\n",
    "    so that any time the LLM generates a message, the validation function is run on it. If\n",
    "    the validation fails, the method will retry the LLM with a fallback strategy, the simplest\n",
    "    being just to add a message to the output with the validation errors and a request to fix them.\n",
    "\n",
    "    The resulting runnable expects a list of messages as input and returns a single AI message.\n",
    "    By default, the LLM can optionally NOT invoke tools, making this easier to incorporate into\n",
    "    your existing chat bot. You can specify a tool_choice to force the validator to be run on\n",
    "    the outputs.\n",
    "\n",
    "    Args:\n",
    "        llm (Runnable): The llm that will generate the initial messages (and optionally fallba)\n",
    "        validator (ValidationNode): The validation logic.\n",
    "        retry_strategy (RetryStrategy): The retry strategy to use.\n",
    "            Possible keys:\n",
    "            - max_attempts: The maximum number of attempts to make.\n",
    "            - fallback: The LLM or function to use in case of validation failure.\n",
    "            - aggregate_messages: A function to aggregate the messages over multiple turns.\n",
    "                Defaults to fetching the last AI message.\n",
    "        tool_choice: If provided, always run the validator on the tool output.\n",
    "\n",
    "    Returns:\n",
    "        Runnable: A runnable that can be invoked with a list of messages and returns a single AI message.\n",
    "    \"\"\"\n",
    "\n",
    "    def add_or_overwrite_messages(left: list, right: Union[list, dict]) -> list:\n",
    "        \"\"\"Append messages. If the update is a 'finalized' output, replace the whole list.\"\"\"\n",
    "        if isinstance(right, dict) and \"finalize\" in right:\n",
    "            finalized = right[\"finalize\"]\n",
    "            if not isinstance(finalized, list):\n",
    "                finalized = [finalized]\n",
    "            for m in finalized:\n",
    "                if m.id is None:\n",
    "                    m.id = str(uuid.uuid4())\n",
    "            return finalized\n",
    "        res = add_messages(left, right)\n",
    "        if not isinstance(res, list):\n",
    "            return [res]\n",
    "        return res\n",
    "\n",
    "    class State(TypedDict):\n",
    "        messages: Annotated[list, add_or_overwrite_messages]\n",
    "        attempt_number: Annotated[int, operator.add]\n",
    "        initial_num_messages: int\n",
    "        input_format: Literal[\"list\", \"dict\"]\n",
    "\n",
    "    builder = StateGraph(State)\n",
    "\n",
    "    def dedict(x: State) -> list:\n",
    "        \"\"\"Get the messages from the state.\"\"\"\n",
    "        return x[\"messages\"]\n",
    "\n",
    "    model = dedict | llm | (lambda msg: {\"messages\": [msg], \"attempt_number\": 1})\n",
    "    fbrunnable = retry_strategy.get(\"fallback\")\n",
    "    if fbrunnable is None:\n",
    "        fb_runnable = llm\n",
    "    elif isinstance(fbrunnable, Runnable):\n",
    "        fb_runnable = fbrunnable  # type: ignore\n",
    "    else:\n",
    "        fb_runnable = RunnableLambda(fbrunnable)\n",
    "    fallback = (\n",
    "        dedict | fb_runnable | (lambda msg: {\"messages\": [msg], \"attempt_number\": 1})\n",
    "    )\n",
    "\n",
    "    def count_messages(state: State) -> dict:\n",
    "        return {\"initial_num_messages\": len(state.get(\"messages\", []))}\n",
    "\n",
    "    builder.add_node(\"count_messages\", count_messages)\n",
    "    builder.add_node(\"llm\", model)\n",
    "    builder.add_node(\"fallback\", fallback)\n",
    "\n",
    "    # To support patch-based retries, we need to be able to\n",
    "    # aggregate the messages over multiple turns.\n",
    "    # The next sequence selects only the relevant messages\n",
    "    # and then applies the validator\n",
    "    select_messages = retry_strategy.get(\"aggregate_messages\") or _default_aggregator\n",
    "\n",
    "    def select_generated_messages(state: State) -> list:\n",
    "        \"\"\"Select only the messages generated within this loop.\"\"\"\n",
    "        selected = state[\"messages\"][state[\"initial_num_messages\"] :]\n",
    "        return [select_messages(selected)]\n",
    "\n",
    "    def endict_validator_output(x: Sequence[AnyMessage]) -> dict:\n",
    "        if tool_choice and not x:\n",
    "            return {\n",
    "                \"messages\": [\n",
    "                    HumanMessage(\n",
    "                        content=f\"ValidationError: please respond with a valid tool call [tool_choice={tool_choice}].\",\n",
    "                        additional_kwargs={\"is_error\": True},\n",
    "                    )\n",
    "                ]\n",
    "            }\n",
    "        return {\"messages\": x}\n",
    "\n",
    "    validator_runnable = select_generated_messages | validator | endict_validator_output\n",
    "    builder.add_node(\"validator\", validator_runnable)\n",
    "\n",
    "    class Finalizer:\n",
    "        \"\"\"Pick the final message to return from the retry loop.\"\"\"\n",
    "\n",
    "        def __init__(self, aggregator: Optional[Callable[[list], AIMessage]] = None):\n",
    "            self._aggregator = aggregator or _default_aggregator\n",
    "\n",
    "        def __call__(self, state: State) -> dict:\n",
    "            \"\"\"Return just the AI message.\"\"\"\n",
    "            initial_num_messages = state[\"initial_num_messages\"]\n",
    "            generated_messages = state[\"messages\"][initial_num_messages:]\n",
    "            return {\n",
    "                \"messages\": {\n",
    "                    \"finalize\": self._aggregator(generated_messages),\n",
    "                }\n",
    "            }\n",
    "\n",
    "    # We only want to emit the final message\n",
    "    builder.add_node(\"finalizer\", Finalizer(retry_strategy.get(\"aggregate_messages\")))\n",
    "\n",
    "    # Define the connectivity\n",
    "    builder.add_edge(START, \"count_messages\")\n",
    "    builder.add_edge(\"count_messages\", \"llm\")\n",
    "\n",
    "    def route_validator(state: State):\n",
    "        if state[\"messages\"][-1].tool_calls or tool_choice is not None:\n",
    "            return \"validator\"\n",
    "        return END\n",
    "\n",
    "    builder.add_conditional_edges(\"llm\", route_validator, [\"validator\", END])\n",
    "    builder.add_edge(\"fallback\", \"validator\")\n",
    "    max_attempts = retry_strategy.get(\"max_attempts\", 3)\n",
    "\n",
    "    def route_validation(state: State):\n",
    "        if state[\"attempt_number\"] > max_attempts:\n",
    "            raise ValueError(\n",
    "                f\"Could not extract a valid value in {max_attempts} attempts.\"\n",
    "            )\n",
    "        for m in state[\"messages\"][::-1]:\n",
    "            if m.type == \"ai\":\n",
    "                break\n",
    "            if m.additional_kwargs.get(\"is_error\"):\n",
    "                return \"fallback\"\n",
    "        return \"finalizer\"\n",
    "\n",
    "    builder.add_conditional_edges(\n",
    "        \"validator\", route_validation, [\"finalizer\", \"fallback\"]\n",
    "    )\n",
    "\n",
    "    builder.add_edge(\"finalizer\", END)\n",
    "\n",
    "    # These functions let the step be used in a MessageGraph\n",
    "    # or a StateGraph with 'messages' as the key.\n",
    "    def encode(x: Union[Sequence[AnyMessage], PromptValue]) -> dict:\n",
    "        \"\"\"Ensure the input is the correct format.\"\"\"\n",
    "        if isinstance(x, PromptValue):\n",
    "            return {\"messages\": x.to_messages(), \"input_format\": \"list\"}\n",
    "        if isinstance(x, list):\n",
    "            return {\"messages\": x, \"input_format\": \"list\"}\n",
    "        raise ValueError(f\"Unexpected input type: {type(x)}\")\n",
    "\n",
    "    def decode(x: State) -> AIMessage:\n",
    "        \"\"\"Ensure the output is in the expected format.\"\"\"\n",
    "        return x[\"messages\"][-1]\n",
    "\n",
    "    return (\n",
    "        encode | builder.compile().with_config(run_name=\"ValidationGraph\") | decode\n",
    "    ).with_config(run_name=\"ValidateWithRetries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40990e2f-db34-4673-b72b-07d9c2f8955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bind_validator_with_retries(\n",
    "    llm: BaseChatModel,\n",
    "    *,\n",
    "    tools: list,\n",
    "    tool_choice: Optional[str] = None,\n",
    "    max_attempts: int = 3,\n",
    ") -> Runnable[Union[List[AnyMessage], PromptValue], AIMessage]:\n",
    "    \"\"\"Binds validators + retry logic ensure validity of generated tool calls.\n",
    "\n",
    "    LLMs that support tool calling are good at generating structured JSON. However, they may\n",
    "    not always perfectly follow your requested schema, especially if the schema is nested or\n",
    "    has complex validation rules. This method allows you to bind a validation function to\n",
    "    the LLM's output, so that any time the LLM generates a message, the validation function\n",
    "    is run on it. If the validation fails, the method will retry the LLM with a fallback\n",
    "    strategy, the simples being just to add a message to the output with the validation\n",
    "    errors and a request to fix them.\n",
    "\n",
    "    The resulting runnable expects a list of messages as input and returns a single AI message.\n",
    "    By default, the LLM can optionally NOT invoke tools, making this easier to incorporate into\n",
    "    your existing chat bot. You can specify a tool_choice to force the validator to be run on\n",
    "    the outputs.\n",
    "\n",
    "    Args:\n",
    "        llm (Runnable): The llm that will generate the initial messages (and optionally fallba)\n",
    "        validator (ValidationNode): The validation logic.\n",
    "        retry_strategy (RetryStrategy): The retry strategy to use.\n",
    "            Possible keys:\n",
    "            - max_attempts: The maximum number of attempts to make.\n",
    "            - fallback: The LLM or function to use in case of validation failure.\n",
    "            - aggregate_messages: A function to aggregate the messages over multiple turns.\n",
    "                Defaults to fetching the last AI message.\n",
    "        tool_choice: If provided, always run the validator on the tool output.\n",
    "\n",
    "    Returns:\n",
    "        Runnable: A runnable that can be invoked with a list of messages and returns a single AI message.\n",
    "    \"\"\"\n",
    "    bound_llm = llm.bind_tools(tools, tool_choice=tool_choice)\n",
    "    retry_strategy = RetryStrategy(max_attempts=max_attempts)\n",
    "    validator = ValidationNode(tools)\n",
    "    return _bind_validator_with_retries(\n",
    "        bound_llm,\n",
    "        validator=validator,\n",
    "        tool_choice=tool_choice,\n",
    "        retry_strategy=retry_strategy,\n",
    "    ).with_config(metadata={\"retry_strategy\": \"default\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ca9d9e2-7288-494f-a3a4-2b8dc19c8bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "\n",
    "class Respond(BaseModel):\n",
    "    \"\"\"Use to generate the response. Always use when responding to the user\"\"\"\n",
    "\n",
    "    reason: str = Field(description=\"Step-by-step justification for the answer.\")\n",
    "    answer: str\n",
    "\n",
    "    @field_validator(\"answer\")\n",
    "    def reason_contains_apology(cls, answer: str):\n",
    "        if \"llama\" not in answer.lower():\n",
    "            raise ValueError(\n",
    "                \"You MUST start with a gimicky, rhyming advertisement for using a Llama V3 (an LLM) in your **answer** field.\"\n",
    "                \" Must be an instant hit. Must be weaved into the answer.\"\n",
    "            )\n",
    "\n",
    "\n",
    "tools = [Respond]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3289acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Or you can use ChatGroq, ChatOpenAI, ChatGoogleGemini, ChatCohere, etc.\n",
    "# See https://python.langchain.com/docs/integrations/chat/ for more info on tool calling\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", api_key=OPENAI_API_KEY)\n",
    "bound_llm = bind_validator_with_retries(llm, tools=tools)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Respond directly by calling the Respond function.\"),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | bound_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1678f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = chain.invoke({\"messages\": [(\"user\", \"Does P = NP?\")]})\n",
    "results.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "945accd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class OutputFormat(BaseModel):\n",
    "    sources: str = Field(\n",
    "        ...,\n",
    "        description=\"The raw transcript / span you could cite to justify the choice.\",\n",
    "    )\n",
    "    content: str = Field(..., description=\"The chosen value.\")\n",
    "\n",
    "\n",
    "class Moment(BaseModel):\n",
    "    quote: str = Field(..., description=\"The relevant quote from the transcript.\")\n",
    "    description: str = Field(..., description=\"A description of the moment.\")\n",
    "    expressed_preference: OutputFormat = Field(\n",
    "        ..., description=\"The preference expressed in the moment.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class BackgroundInfo(BaseModel):\n",
    "    factoid: OutputFormat = Field(\n",
    "        ..., description=\"Important factoid about the member.\"\n",
    "    )\n",
    "    professions: list\n",
    "    why: str = Field(..., description=\"Why this is important.\")\n",
    "\n",
    "\n",
    "class KeyMoments(BaseModel):\n",
    "    topic: str = Field(..., description=\"The topic of the key moments.\")\n",
    "    happy_moments: List[Moment] = Field(\n",
    "        ..., description=\"A list of key moments related to the topic.\"\n",
    "    )\n",
    "    tense_moments: List[Moment] = Field(\n",
    "        ..., description=\"Moments where things were a bit tense.\"\n",
    "    )\n",
    "    sad_moments: List[Moment] = Field(\n",
    "        ..., description=\"Moments where things where everyone was downtrodden.\"\n",
    "    )\n",
    "    background_info: list[BackgroundInfo]\n",
    "    moments_summary: str = Field(..., description=\"A summary of the key moments.\")\n",
    "\n",
    "\n",
    "class Member(BaseModel):\n",
    "    name: OutputFormat = Field(..., description=\"The name of the member.\")\n",
    "    role: Optional[str] = Field(None, description=\"The role of the member.\")\n",
    "    age: Optional[int] = Field(None, description=\"The age of the member.\")\n",
    "    background_details: List[BackgroundInfo] = Field(\n",
    "        ..., description=\"A list of background details about the member.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class InsightfulQuote(BaseModel):\n",
    "    quote: OutputFormat = Field(\n",
    "        ..., description=\"An insightful quote from the transcript.\"\n",
    "    )\n",
    "    speaker: str = Field(..., description=\"The name of the speaker who said the quote.\")\n",
    "    analysis: str = Field(\n",
    "        ..., description=\"An analysis of the quote and its significance.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class TranscriptMetadata(BaseModel):\n",
    "    title: str = Field(..., description=\"The title of the transcript.\")\n",
    "    location: OutputFormat = Field(\n",
    "        ..., description=\"The location where the interview took place.\"\n",
    "    )\n",
    "    duration: str = Field(..., description=\"The duration of the interview.\")\n",
    "\n",
    "\n",
    "class TranscriptSummary(BaseModel):\n",
    "    metadata: TranscriptMetadata = Field(\n",
    "        ..., description=\"Metadata about the transcript.\"\n",
    "    )\n",
    "    participants: List[Member] = Field(\n",
    "        ..., description=\"A list of participants in the interview.\"\n",
    "    )\n",
    "    key_moments: List[KeyMoments] = Field(\n",
    "        ..., description=\"A list of key moments from the interview.\"\n",
    "    )\n",
    "    insightful_quotes: List[InsightfulQuote] = Field(\n",
    "        ..., description=\"A list of insightful quotes from the interview.\"\n",
    "    )\n",
    "    overall_summary: str = Field(\n",
    "        ..., description=\"An overall summary of the interview.\"\n",
    "    )\n",
    "    next_steps: List[str] = Field(\n",
    "        ..., description=\"A list of next steps or action items based on the interview.\"\n",
    "    )\n",
    "    other_stuff: List[OutputFormat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2e7ad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = [\n",
    "    (\n",
    "        \"Pete\",\n",
    "        \"Hey Xu, Laura, thanks for hopping on this call. I've been itching to talk about this Drake and Kendrick situation.\",\n",
    "    ),\n",
    "    (\n",
    "        \"Xu\",\n",
    "        \"No problem. As its my job, I've got some thoughts on this beef.\",\n",
    "    ),\n",
    "    (\n",
    "        \"Laura\",\n",
    "        \"Yeah, I've got some insider info so this should be interesting.\",\n",
    "    ),\n",
    "    (\"Pete\", \"Dope. So, when do you think this whole thing started?\"),\n",
    "    (\n",
    "        \"Pete\",\n",
    "        \"Definitely was Kendrick's 'Control' verse that kicked it off.\",\n",
    "    ),\n",
    "    (\n",
    "        \"Laura\",\n",
    "        \"Truth, but Drake never went after him directly. Just some subtle jabs here and there.\",\n",
    "    ),\n",
    "    (\n",
    "        \"Xu\",\n",
    "        \"That's the thing with beefs like this, though. They've always been a a thing, pushing artists to step up their game.\",\n",
    "    ),\n",
    "    (\n",
    "        \"Pete\",\n",
    "        \"For sure, and this beef has got the fans taking sides. Some are all about Drake's mainstream appeal, while others are digging Kendrick's lyrical skills.\",\n",
    "    ),\n",
    "    (\n",
    "        \"Laura\",\n",
    "        \"I mean, Drake knows how to make a hit that gets everyone hyped. That's his thing.\",\n",
    "    ),\n",
    "    (\n",
    "        \"Pete\",\n",
    "        \"I hear you, Laura, but I gotta give it to Kendrick when it comes to straight-up bars. The man's a beast on the mic.\",\n",
    "    ),\n",
    "    (\n",
    "        \"Xu\",\n",
    "        \"It's wild how this beef is shaping fans.\",\n",
    "    ),\n",
    "    (\"Pete\", \"do you think these beefs can actually be good for hip-hop?\"),\n",
    "    (\n",
    "        \"Xu\",\n",
    "        \"Hell yeah, Pete. When it's done right, a beef can push the genre forward and make artists level up.\",\n",
    "    ),\n",
    "    (\"Laura\", \"eh\"),\n",
    "    (\"Pete\", \"So, where do you see this beef going?\"),\n",
    "    (\n",
    "        \"Laura\",\n",
    "        \"Honestly, I think it'll stay a hot topic for the fans, but unless someone drops a straight-up diss track, it's not gonna escalate.\",\n",
    "    ),\n",
    "    (\"Laura\", \"ehhhhhh not sure\"),\n",
    "    (\n",
    "        \"Pete\",\n",
    "        \"I feel that. I just want both of them to keep dropping heat, beef or no beef.\",\n",
    "    ),\n",
    "    (\n",
    "        \"Xu\",\n",
    "        \"I'm curious. May influence a lot of people. Make things more competitive. Bring on a whole new wave of lyricism.\",\n",
    "    ),\n",
    "    (\n",
    "        \"Pete\",\n",
    "        \"Word. Hey, thanks for chopping it up with me, Xu and Laura. This was dope.\",\n",
    "    ),\n",
    "    (\"Xu\", \"Where are you going so fast?\"),\n",
    "    (\n",
    "        \"Laura\",\n",
    "        \"For real, I had a good time. Nice to get different perspectives on the situation.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "formatted = \"\\n\".join(f\"{x[0]}: {x[1]}\" for x in transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45028fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [TranscriptSummary]\n",
    "bound_llm = bind_validator_with_retries(\n",
    "    llm,\n",
    "    tools=tools,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Respond directly using the TranscriptSummary function.\"),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | bound_llm\n",
    "\n",
    "try:\n",
    "    results = chain.invoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                (\n",
    "                    \"user\",\n",
    "                    f\"Extract the summary from the following conversation:\\n\\n<convo>\\n{formatted}\\n</convo>\"\n",
    "                    \"\\n\\nRemember to respond using the TranscriptSummary function.\",\n",
    "                )\n",
    "            ]\n",
    "        },\n",
    "    )\n",
    "    results.pretty_print()\n",
    "except ValueError as e:\n",
    "    print(repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d648b954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
